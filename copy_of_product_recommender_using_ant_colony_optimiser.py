# -*- coding: utf-8 -*-
"""Copy of Product Recommender Using Ant Colony Optimiser

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Y7XLTYOWbB9Ovv0uIQi-EudU90QOscfa

---

# **Main App.Py**
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import pandas as pd
# import numpy as np
# import time
# import os
# import tensorflow as tf
# import matplotlib.pyplot as plt
# 
# from sklearn.feature_extraction.text import TfidfVectorizer
# from sklearn.metrics.pairwise import cosine_similarity
# from sklearn.preprocessing import LabelEncoder, StandardScaler
# from sklearn.model_selection import train_test_split
# from sklearn.metrics import accuracy_score
# 
# from tensorflow.keras.models import Sequential
# from tensorflow.keras.layers import Dense, Dropout
# from tensorflow.keras.optimizers import Adam
# from tensorflow.keras.callbacks import EarlyStopping
# 
# # --------------------------------------------------
# # PAGE CONFIG
# # --------------------------------------------------
# st.set_page_config(
#     page_title="Normal ANN vs ACO Hybrid Recommender",
#     layout="wide"
# )
# 
# st.title("üëó Fashion Recommendation System")
# 
# # --------------------------------------------------
# # DATASET LOADING (ROBUST)
# # --------------------------------------------------
# st.sidebar.header("üìÇ Dataset")
# 
# uploaded_file = st.sidebar.file_uploader(
#     "Upload FashionDataset.csv",
#     type=["csv"]
# )
# 
# @st.cache_data
# def load_data(file):
#     return pd.read_csv(file).dropna()
# 
# df = None
# if uploaded_file:
#     df = load_data(uploaded_file)
# elif os.path.exists("FashionDataset.csv"):
#     df = load_data("FashionDataset.csv")
# else:
#     st.warning("‚ö† Upload FashionDataset.csv to continue")
#     st.stop()
# 
# # --------------------------------------------------
# # SEARCH BAR
# # --------------------------------------------------
# query = st.text_input("üîç Search for a product")
# 
# TEXT_COLS = [c for c in df.columns if df[c].dtype == "object"]
# 
# # --------------------------------------------------
# # TF-IDF (ACO ONLY)
# # --------------------------------------------------
# def build_corpus(df):
#     return df[TEXT_COLS].astype(str).agg(" ".join, axis=1)
# 
# @st.cache_resource
# def build_tfidf(corpus):
#     vec = TfidfVectorizer(stop_words="english", max_features=2000)
#     mat = vec.fit_transform(corpus)
#     return vec, mat
# 
# corpus = build_corpus(df)
# tfidf_vec, tfidf_mat = build_tfidf(corpus)
# 
# # --------------------------------------------------
# # DATA PREPARATION
# # --------------------------------------------------
# target_col = TEXT_COLS[0]
# 
# X = df.drop(columns=[target_col])
# y = df[target_col]
# 
# for c in X.columns:
#     if X[c].dtype == "object":
#         X[c] = LabelEncoder().fit_transform(X[c])
# 
# y_le = LabelEncoder()
# y_enc = y_le.fit_transform(y)
# 
# scaler = StandardScaler()
# X_scaled = scaler.fit_transform(X)
# 
# X_train, X_test, y_train, y_test = train_test_split(
#     X_scaled, y_enc, test_size=0.2, random_state=42
# )
# 
# NUM_CLASSES = len(np.unique(y_enc))
# 
# # --------------------------------------------------
# # ANN MODEL
# # --------------------------------------------------
# def build_ann(neurons, lr):
#     model = Sequential([
#         Dense(neurons, activation="relu", input_shape=(X_train.shape[1],)),
#         Dropout(0.3),
#         Dense(neurons // 2, activation="relu"),
#         Dense(NUM_CLASSES, activation="softmax")
#     ])
#     model.compile(
#         optimizer=Adam(lr),
#         loss="sparse_categorical_crossentropy",
#         metrics=["accuracy"]
#     )
#     return model
# 
# early_stop = EarlyStopping(
#     monitor="val_loss",
#     patience=3,
#     restore_best_weights=True
# )
# 
# # --------------------------------------------------
# # NORMAL ANN
# # --------------------------------------------------
# def train_normal_ann():
#     tf.keras.backend.clear_session()
# 
#     model = build_ann(32, 0.001)
#     model.fit(
#         X_train, y_train,
#         epochs=15,
#         batch_size=32,
#         validation_split=0.2,
#         callbacks=[early_stop],
#         verbose=0
#     )
# 
#     preds = np.argmax(model.predict(X_test, verbose=0), axis=1)
#     acc = accuracy_score(y_test, preds)
# 
#     return model, acc
# 
# # --------------------------------------------------
# # ACO OPTIMIZATION (FIXED & ACCURATE)
# # --------------------------------------------------
# NEURON_SPACE = [16, 32, 64]
# LR_SPACE = [0.01, 0.001]
# BATCH_SPACE = [32, 64]
# 
# def run_aco(iterations=3, ants=4):
#     pheromone = np.ones((len(NEURON_SPACE),
#                           len(LR_SPACE),
#                           len(BATCH_SPACE)))
# 
#     best_val_acc = 0
#     best_params = None
#     acc_history = []
# 
#     for _ in range(iterations):
#         for _ in range(ants):
#             tf.keras.backend.clear_session()
# 
#             probs = pheromone / pheromone.sum()
#             idx = np.unravel_index(
#                 np.random.choice(probs.size, p=probs.ravel()),
#                 probs.shape
#             )
# 
#             n = NEURON_SPACE[idx[0]]
#             lr = LR_SPACE[idx[1]]
#             b = BATCH_SPACE[idx[2]]
# 
#             model = build_ann(n, lr)
#             history = model.fit(
#                 X_train, y_train,
#                 epochs=12,
#                 batch_size=b,
#                 validation_split=0.2,
#                 callbacks=[early_stop],
#                 verbose=0
#             )
# 
#             val_acc = max(history.history["val_accuracy"])
#             acc_history.append(val_acc)
# 
#             pheromone[idx] += val_acc * 2
# 
#             if val_acc > best_val_acc:
#                 best_val_acc = val_acc
#                 best_params = (n, lr, b)
# 
#             del model
# 
#     tf.keras.backend.clear_session()
# 
#     final_model = build_ann(best_params[0], best_params[1])
#     final_model.fit(
#         X_train, y_train,
#         epochs=15,
#         batch_size=best_params[2],
#         validation_split=0.2,
#         callbacks=[early_stop],
#         verbose=0
#     )
# 
#     preds = np.argmax(final_model.predict(X_test, verbose=0), axis=1)
#     test_acc = accuracy_score(y_test, preds)
# 
#     return final_model, best_params, test_acc, acc_history
# 
# # --------------------------------------------------
# # RECOMMENDATION FUNCTIONS
# # --------------------------------------------------
# def normal_ann_recommend(model, top_k=5):
#     preds = np.argmax(model.predict(X_scaled, verbose=0), axis=1)
#     majority_class = np.bincount(preds).argmax()
#     return df[y_enc == majority_class].head(top_k)
# 
# def aco_ann_recommend(query, model, top_k=5):
#     q_vec = tfidf_vec.transform([query])
#     sim = cosine_similarity(q_vec, tfidf_mat).flatten()
# 
#     top_idx = sim.argsort()[-30:][::-1]
#     pred_class = np.argmax(
#         model.predict(X_scaled[top_idx[0]].reshape(1, -1), verbose=0)
#     )
# 
#     filtered = [i for i in top_idx if y_enc[i] == pred_class]
#     if not filtered:
#         filtered = top_idx[:top_k]
# 
#     return df.iloc[filtered[:top_k]]
# 
# # --------------------------------------------------
# # TRAIN BUTTONS
# # --------------------------------------------------
# st.markdown("---")
# col1, col2 = st.columns(2)
# 
# with col1:
#     if st.button("üîµ Train Normal ANN"):
#         model, acc = train_normal_ann()
#         st.session_state["normal_model"] = model
#         st.session_state["normal_acc"] = acc
#         st.success("Normal ANN trained")
# 
# with col2:
#     if st.button("üêú Train ACO ANN"):
#         model, params, acc, hist = run_aco()
#         st.session_state["aco_model"] = model
#         st.session_state["aco_acc"] = acc
#         st.session_state["aco_hist"] = hist
#         st.success("ACO ANN trained")
# 
# # --------------------------------------------------
# # METRICS & VISUALS
# # --------------------------------------------------
# if "normal_acc" in st.session_state and "aco_acc" in st.session_state:
#     st.markdown("---")
#     col1, col2 = st.columns(2)
# 
#     col1.metric("Normal ANN Accuracy", f"{st.session_state['normal_acc']:.3f}")
#     col2.metric("ACO ANN Accuracy", f"{st.session_state['aco_acc']:.3f}")
# 
#     fig, ax = plt.subplots()
#     ax.plot(st.session_state["aco_hist"])
#     ax.set_title("ACO Convergence")
#     ax.set_ylabel("Validation Accuracy")
#     st.pyplot(fig)
# 
# # --------------------------------------------------
# # RECOMMENDATIONS
# # --------------------------------------------------
# if query and "normal_model" in st.session_state and "aco_model" in st.session_state:
#     st.markdown("---")
#     col1, col2 = st.columns(2)
# 
#     with col1:
#         st.subheader("üîµ Normal ANN")
#         st.dataframe(normal_ann_recommend(st.session_state["normal_model"]))
# 
#     with col2:
#         st.subheader("üêú ACO Hybrid (TF-IDF + ANN)")
#         st.dataframe(aco_ann_recommend(query, st.session_state["aco_model"]))
# 
# # --------------------------------------------------
# # FOOTER
# # --------------------------------------------------
# st.markdown("""
# ---
# ### ‚úî Summary
# - **Normal ANN**: Pure classification-based recommendation
# - **ACO Model**: TF-IDF candidate selection + ANN classification
# - **ACO improves accuracy via pheromone-guided hyperparameter tuning**
# """)
#

!pip install streamlit
!pip install numpy
!pip install pandas
!pip install yfinance
!pip install matplotlib
!pip install scikit-learn
!pip install tensorflow
!pip install pyswarm
!pip install pyngrok

!ngrok config add-authtoken 2y1h90IDQ09WkydpMkH0V1uwDoY_48ETKKBgkK1u6cGDHcwm2

from pyngrok import ngrok
import subprocess
import threading
import time

ngrok.kill()
def run_streamlit():
    subprocess.call(["streamlit", "run", "app.py", "--server.port=8503",
                     "--server.headless=true", "--server.enableCORS=false"])
thread = threading.Thread(target=run_streamlit)
thread.start()
time.sleep(5)

public_url = ngrok.connect(8503)
print(f"Streamlit Application Link: {public_url}")

ngrok.kill()